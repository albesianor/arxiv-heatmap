{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22549bc0",
   "metadata": {},
   "source": [
    "# arXiv heatmap\n",
    "### Data cleaning\n",
    "\n",
    "##### Starting point\n",
    "- the arXiv metadata stripped of all columns except for `id` (`string`), `update_date` (`datetime`), and `categories` (`list`): `data/arxiv-metadata-id-categories.parquet`\n",
    "- the list of all current categories: `data/arxiv-categories.json`\n",
    "\n",
    "##### End goal\n",
    "A `pandas` dataframe:\n",
    "- indexed by `update_date`\n",
    "- with columns the 2x2 combinations of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626dbf9",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f84df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "from itertools import combinations_with_replacement as cwr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4c75b",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce7b99",
   "metadata": {},
   "source": [
    "First we import the list of current arXiv category tags and store it in the list `arxiv_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e84da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/arxiv-categories.json', 'r') as f:\n",
    "    arxiv_categories_descriptions = json.load(f)\n",
    "\n",
    "arxiv_categories = [cat['tag'] for cat in arxiv_categories_descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df65dc",
   "metadata": {},
   "source": [
    "Now we import the stripped data as `arxiv_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf8a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_metadata = pd.read_parquet('../data/arxiv-metadata-id-date-categories.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73904746",
   "metadata": {},
   "source": [
    "The arXiv categories changed over the years: we find all categories that are not the current ones and store them in the set `missing_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214d84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alg-geom', 'dg-ga', 'chem-ph', 'plasm-ph', 'ao-sci', 'mtrl-th', 'funct-an', 'comp-gas', 'q-alg', 'cond-mat', 'acc-phys', 'astro-ph', 'atom-ph', 'supr-con', 'chao-dyn', 'bayes-an', 'cmp-lg', 'q-bio', 'patt-sol', 'adap-org', 'solv-int'}\n"
     ]
    }
   ],
   "source": [
    "missing_categories = set()\n",
    "\n",
    "for index, row in arxiv_metadata.iterrows():\n",
    "    for category in row['categories']:\n",
    "        if category not in arxiv_categories:\n",
    "            missing_categories.add(category)\n",
    "\n",
    "print(missing_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3d8e0",
   "metadata": {},
   "source": [
    "We need to decide what to do for each of the missing categories.  The most reasonable choice to me seems to find the closest matching current category and replace each missing category with that.\n",
    "\n",
    "| Old        |  New                | To add? |\n",
    "| ---------- | ------------------- | :-----: |\n",
    "| `mtrl-th`  | `cond-mat.mtrl-sci` |         |\n",
    "| `q-bio`    | ---                 | X       |\n",
    "| `acc-phys` | `physics.acc-ph`    |         |\n",
    "| `dg-ga`    | `math.DG`           |         |\n",
    "| `cond-mat` | ---                 | X       |\n",
    "| `chem-ph`  | `physics.chem-ph`   |         |\n",
    "| `astro-ph` | ---                 | X       |\n",
    "| `comp-gas` | `nlin.CG`           |         |\n",
    "| `funct-an` | `math.FA`           |         |\n",
    "| `patt-sol` | `nlin.PS`           |         |\n",
    "| `solv-int` | `nlin.SI`           |         |\n",
    "| `alg-geom` | `math.AG`           |         |\n",
    "| `adap-org` | `nlin.AO`           |         |\n",
    "| `supr-con` | `cond-mat.supr-con` |         |\n",
    "| `plasm-ph` | `physics.plasm-ph`  |         |\n",
    "| `chao-dyn` | `nlin.CD`           |         |\n",
    "| `bayes-an` | `physics.data-an`   |         |\n",
    "| `q-alg`    | `math.QA`           |         |\n",
    "| `ao-sci`   | `physics.ao-ph`     |         |\n",
    "| `atom-ph`  | `physics.atom-ph`   |         |\n",
    "| `cmp-lg`   | `cs.CL`             |         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164d30b",
   "metadata": {},
   "source": [
    "The categories `q-bio`, `cond-mat`, and `astro-ph` have been over the years split into subcategories.  Hence, some preprints are classified into what are now meta-categories.  We add these three categories, and we will use them only for those preprints dating to before the splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579ee69",
   "metadata": {},
   "source": [
    "We then create a list `graph_edges_keys` of tuples (with repetitions) of `arxiv_categories_extra`: they will be keys for a dictionary whose entries represent the daily entries in that cross-listing (the tuple with repetition are the papers listed in only one category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbf89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_categories_extra = arxiv_categories + ['q-bio', 'cond-mat', 'astro-ph']\n",
    "arxiv_categories_combinations = cwr(arxiv_categories_extra, 2)\n",
    "\n",
    "# use sorted to make sure the tuples are in a consistent ordering\n",
    "graph_edges_keys = [tuple(sorted(index)) for index in arxiv_categories_combinations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead670c1",
   "metadata": {},
   "source": [
    "Now `arxiv_categories_extra` contains the current categories plus the three legacy meta-categories, and `graph_edges` has been updated to include the three new nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8daf74",
   "metadata": {},
   "source": [
    "The goal now is to go through `arxiv_metadata` again and replace the missing categories with the new ones.  We start by creating a dictionary `cat_dictionary` to map old categories to new categories, and a function `cat_translator` to translate a list of categories to the new ones using the dictionary (removing duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bb19bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dictionary = {\n",
    "    'alg-geom': 'math.AG',\n",
    "    'dg-ga': 'math.DG',\n",
    "    'chem-ph': 'physics.chem-ph',\n",
    "    'plasm-ph': 'physics.plasm-ph',\n",
    "    'ao-sci': 'physics.ao-ph',\n",
    "    'mtrl-th': 'cond-mat.mtrl-sci',\n",
    "    'funct-an': 'math.FA',\n",
    "    'comp-gas': 'nlin.CG',\n",
    "    'q-alg': 'math.QA',\n",
    "    'acc-phys': 'physics.acc-ph',\n",
    "    'atom-ph': 'physics.atom-ph',\n",
    "    'supr-con': 'cond-mat.supr-con',\n",
    "    'chao-dyn': 'nlin.CD',\n",
    "    'bayes-an': 'physics.data-an',\n",
    "    'cmp-lg': 'cs.CL',\n",
    "    'patt-sol': 'nlin.PS',\n",
    "    'adap-org': 'nlin.AO',\n",
    "    'solv-int': 'nlin.SI'\n",
    "}\n",
    "\n",
    "def cat_translator(categories: 'list') -> 'list':\n",
    "    return sorted(set([cat_dictionary[cat] if cat in cat_dictionary else cat for cat in categories]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9585468",
   "metadata": {},
   "source": [
    "Then we traverse the `categories` column in `arxiv_metadata` and use the dictionary `cat_translator` to update categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fe5fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_metadata['categories'] = arxiv_metadata['categories'].apply(cat_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5af52db",
   "metadata": {},
   "source": [
    "We save the new cleaned file to `data/arxiv-metadata-cleaned.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b2d7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_metadata.to_parquet('../data/arxiv-metadata-cleaned.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943de94",
   "metadata": {},
   "source": [
    "### Data crunching\n",
    "The goal now is to produce a new dataframe, indexed by `update_date` whose rows are the cross-listings.  We also want another dataframe containing the total daily publications in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168eb40",
   "metadata": {},
   "source": [
    "We start by importing the cleaned metadata from `data/arxiv-metadata-cleaned.parquet` to `arxiv_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04ea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_metadata = pd.read_parquet('../data/arxiv-metadata-cleaned.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5dc57",
   "metadata": {},
   "source": [
    "Next we define a function `take_snapshot` that takes a dataframe of listings for one day and returns a dictionary containing the cross listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18702633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_snapshot(group: pd.Series) -> dict:\n",
    "    graph_edges = dict.fromkeys(graph_edges_keys, 0)\n",
    "    for entry in group:\n",
    "        for edge in cwr(entry, 2):\n",
    "            graph_edges[tuple(sorted(edge))] += 1\n",
    "    return graph_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f3365",
   "metadata": {},
   "source": [
    "Now we create a dataframe `arxiv_snapshots` containing the daily snapshots of arXiv cross-listings.  The new dataframe is obtained by grouping `arxiv_metadata` by `update_date` and aggregating each group via the `take_snapshot` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99567cf7",
   "metadata": {},
   "source": [
    "We start by creating a dataframe `arxiv_snapshot` with dict entries in categories, representing the graph for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b46ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots = arxiv_metadata.drop(columns=['id']).groupby('update_date').agg({'categories': take_snapshot})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cd786",
   "metadata": {},
   "source": [
    "Next we reset the index of `arxiv_snapshot_dicts` so that the `update_date` becomes a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b3f5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53055de2",
   "metadata": {},
   "source": [
    "Then we pop the `categories` column, we explode it into its components, and join it to `arxiv_snapshots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70d2ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots = arxiv_snapshots.join(pd.DataFrame(arxiv_snapshots.pop('categories').tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49378547",
   "metadata": {},
   "source": [
    "Finally, we re-index `arxiv_snapshots` to `update_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22bcf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots.set_index('update_date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13f347",
   "metadata": {},
   "source": [
    "Now we save the snapshots to `data/arxiv-snapshots.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e663641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ralbesia/Desktop/Coding/Projects/arxiv-heatmap/.conda/lib/python3.11/site-packages/pandas/io/parquet.py:190: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "arxiv_snapshots.to_parquet('../data/arxiv-snapshots.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae703de8",
   "metadata": {},
   "source": [
    "**Note:** the first date in `arxiv_snapshots` seems to contain all entries before May 23, 2007.  We should  drop it when we do the analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
