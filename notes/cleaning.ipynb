{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22549bc0",
   "metadata": {},
   "source": [
    "# arXiv heatmap\n",
    "### Data cleaning\n",
    "\n",
    "##### Starting point\n",
    "- the arXiv metadata stripped of all columns except for `id` (`string`), `update_date` (`datetime`), and `categories` (`list`): `data/arxiv-metadata-id-categories.parquet`\n",
    "- the list of all current categories: `data/arxiv-categories.json`\n",
    "\n",
    "##### End goal\n",
    "A `pandas` dataframe:\n",
    "- indexed by `update_date`\n",
    "- with columns the 2x2 combinations of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626dbf9",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f84df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "from itertools import combinations_with_replacement as cwr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4c75b",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce7b99",
   "metadata": {},
   "source": [
    "First we import the list of current arXiv category tags and store it in the list `arxiv_categories`.\n",
    "We also create a dictionary `graph_edges` whose keys are the tuples (with repetitions) of `arxiv_categories` and whose entries represent the daily entries in that cross-listing (the tuple with repetition are the papers listed in only one category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e84da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/arxiv-categories.json', 'r') as f:\n",
    "    arxiv_categories_descriptions = json.load(f)\n",
    "\n",
    "arxiv_categories = [cat['tag'] for cat in arxiv_categories_descriptions]\n",
    "arxiv_categories_combinations = list(cwr(arxiv_categories, 2))\n",
    "\n",
    "# use sorted to make sure the tuples are in a consistent ordering\n",
    "graph_edges = {tuple(sorted(index)): 0 for index in arxiv_categories_combinations}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df65dc",
   "metadata": {},
   "source": [
    "Now we import the stripped data as `arxiv_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf8a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_metadata = pd.read_parquet('../data/arxiv-metadata-id-date-categories.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73904746",
   "metadata": {},
   "source": [
    "The arXiv categories changed over the years: we find all categories that are not the current ones and store them in the set `missing_categories`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214d84db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mtrl-th', 'q-bio', 'acc-phys', 'dg-ga', 'cond-mat', 'chem-ph', 'astro-ph', 'comp-gas', 'funct-an', 'patt-sol', 'solv-int', 'alg-geom', 'adap-org', 'supr-con', 'plasm-ph', 'chao-dyn', 'bayes-an', 'q-alg', 'ao-sci', 'atom-ph', 'cmp-lg'}\n"
     ]
    }
   ],
   "source": [
    "missing_categories = set()\n",
    "\n",
    "for index, row in arxiv_metadata.iterrows():\n",
    "    for category in row['categories']:\n",
    "        if category not in arxiv_categories:\n",
    "            missing_categories.add(category)\n",
    "\n",
    "print(missing_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3d8e0",
   "metadata": {},
   "source": [
    "We need to decide what to do for each of the missing categories.  The most reasonable choice to me seems to find the closest matching current category and replace each missing category with that.\n",
    "\n",
    "| Old        |  New    |\n",
    "| ---------- | --------|\n",
    "| `mtrl-th`  ||\n",
    "| `q-bio`    ||\n",
    "| `acc-phys` ||\n",
    "| `dg-ga`    ||\n",
    "| `cond-mat` ||\n",
    "| `chem-ph`  ||\n",
    "| `astro-ph` ||\n",
    "| `comp-gas` ||\n",
    "| `funct-an` ||\n",
    "| `patt-sol` ||\n",
    "| `solv-int` ||\n",
    "| `alg-geom` ||\n",
    "| `adap-org` ||\n",
    "| `supr-con` ||\n",
    "| `plasm-ph` ||\n",
    "| `chao-dyn` ||\n",
    "| `bayes-an` ||\n",
    "| `q-alg`    ||\n",
    "| `ao-sci`   ||\n",
    "| `atom-ph`  ||\n",
    "| `cmp-lg`   ||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8daf74",
   "metadata": {},
   "source": [
    "Now we go through `arxiv_metadata` again and replace the missing categories with the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb19bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943de94",
   "metadata": {},
   "source": [
    "### Data crunching\n",
    "The goal now is to produce a new dataframe, indexed by `update-data` whose rows are the cross-listings.  We also want another dataframe containing the total daily publications in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f04ea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
