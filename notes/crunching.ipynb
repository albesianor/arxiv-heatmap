{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5be00b9",
   "metadata": {},
   "source": [
    "# arXiv heatmap\n",
    "\n",
    "### Data crunching\n",
    "##### Starting point\n",
    "- the cleaned arXiv metadata: `data/arxiv-metadata-cleaned.parquet`\n",
    "- the list of all current categories: `data/arxiv-categories.json`\n",
    "\n",
    "##### End goal\n",
    "A `pandas` dataframe:\n",
    "- indexed by `update_date`\n",
    "- with columns the 2x2 combinations of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43a5f9",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699b2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581fcfed",
   "metadata": {},
   "source": [
    "We start by importing the cleaned metadata from `data/arxiv-metadata-cleaned.parquet` to `arxiv_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed85728",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_metadata = pd.read_parquet('../data/arxiv-metadata-cleaned.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e6137",
   "metadata": {},
   "source": [
    "We also recreate the graph edges, including the extra categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bfe8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement as cwr\n",
    "\n",
    "with open('../data/arxiv-categories.json', 'r') as f:\n",
    "    arxiv_categories_descriptions = json.load(f)\n",
    "\n",
    "arxiv_categories = sorted([cat['tag'] for cat in arxiv_categories_descriptions] + ['q-bio', 'cond-mat', 'astro-ph'])\n",
    "\n",
    "arxiv_categories_combinations = cwr(arxiv_categories, 2)\n",
    "\n",
    "# use sorted to make sure the tuples are in a consistent ordering\n",
    "graph_edges_keys = [tuple(sorted(index)) for index in arxiv_categories_combinations]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953806f5",
   "metadata": {},
   "source": [
    "### Dataframe of cross-listings\n",
    "The goal of this section is to produce a new dataframe, indexed by `update_date` whose rows are the cross-listings.  We also want another dataframe containing the total daily publications in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22288048",
   "metadata": {},
   "source": [
    "We begin by defining a function `take_snapshot` that takes a dataframe of listings for one day and returns a dictionary containing the cross listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913c1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_snapshot(group: pd.Series) -> dict:\n",
    "    graph_edges = dict.fromkeys(graph_edges_keys, 0)\n",
    "    for entry in group:\n",
    "        for edge in cwr(entry, 2):\n",
    "            graph_edges[tuple(sorted(edge))] += 1\n",
    "    return graph_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84e367",
   "metadata": {},
   "source": [
    "Now we create a dataframe `arxiv_snapshots` containing the daily snapshots of arXiv cross-listings.  The new dataframe is obtained by grouping `arxiv_metadata` by `update_date` and aggregating each group via the `take_snapshot` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0dda3a",
   "metadata": {},
   "source": [
    "We start by creating a dataframe `arxiv_snapshot` with dict entries in categories, representing the graph for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots = arxiv_metadata.drop(columns=['id']).groupby('update_date').agg({'categories': take_snapshot})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bf4b19",
   "metadata": {},
   "source": [
    "Next we reset the index of `arxiv_snapshot_dicts` so that the `update_date` becomes a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d040f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93ac40",
   "metadata": {},
   "source": [
    "Then we pop the `categories` column, we explode it into its components, and join it to `arxiv_snapshots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31551151",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots = arxiv_snapshots.join(pd.DataFrame(arxiv_snapshots.pop('categories').tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d71a1",
   "metadata": {},
   "source": [
    "Finally, we re-index `arxiv_snapshots` to `update_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4578dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_snapshots.set_index('update_date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f6365",
   "metadata": {},
   "source": [
    "Now we save the snapshots to `data/arxiv-snapshots.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7595e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ralbesia/Desktop/Coding/Projects/arxiv-heatmap/.conda/lib/python3.11/site-packages/pandas/io/parquet.py:190: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "arxiv_snapshots.to_parquet('../data/arxiv-snapshots.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bd373",
   "metadata": {},
   "source": [
    "**Note:** the first date in `arxiv_snapshots` seems to contain all entries before May 23, 2007.  We should  drop it when we do the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a6da17",
   "metadata": {},
   "source": [
    "### Dataframe of totals\n",
    "The goal now is to produce another dataframe, indexed by `update_date`, containing the daily totals for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ebde3",
   "metadata": {},
   "source": [
    "We begin by defining a function `take_totals` that takes a dataframe of listings for one day and returns a dictionary containing the totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df2b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_totals(group: pd.Series) -> dict:\n",
    "    daily_totals = dict.fromkeys(arxiv_categories, 0)\n",
    "    for entry in group:\n",
    "        for edge in cwr(entry, 2):\n",
    "            daily_totals[tuple(sorted(edge))] += 1\n",
    "    return daily_totals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
